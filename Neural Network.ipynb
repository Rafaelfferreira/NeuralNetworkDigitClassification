{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "#Main class that represents the network\n",
    "class Network(object):\n",
    "    def __init__(self, sizes): #sizes is a list that contains the number of neurons in each respective layer\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]] #randomly initializing the biases afther the firs layer (because the first one is the input layer)\n",
    "        self.weights = [np.random.rand(y, x) for x, y in zip(sizes[:-1], sizes[1:])] #randomly initializing the weights\n",
    "        \n",
    "    def feedForward(self, a):\n",
    "        \"Return the output of the network if 'a' is input.\"\n",
    "        for b, w in zip(self.biases, self.weights): #zip aggregates iterables into tuples\n",
    "            a = sigmoid(np.dot(w,a)+b) #np.dot = produto vetorial\n",
    "            \n",
    "    def SGD(self, trainingData, epochs, miniBatchSize, eta, testData = None): #Stochastic gradient descent\n",
    "        #eta is the learning rate n\n",
    "        #\"Train the neural network using mini-batch stochastic gradient descent. the trainingData is a list of tuples representing the training inputs and the desired outputs.\"\n",
    "        #\"If testData is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. Thi is useful for tracking progress but it slows things down substantialy\"\n",
    "        if testData: nTest = len(testData)\n",
    "        n = len(trainingData)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(trainingData) #shuffle the order of trainingData\n",
    "            #now that the trainingData has been shuffled we will partition it in miniBatches\n",
    "            miniBatches = [trainingData[k:k+miniBatchSize] for k in range(0, n, miniBatchSize)] #This range goes from 0 to n in miniBatchSize jumps\n",
    "            for miniBatch in miniBatches:\n",
    "                self.updateMiniBatch(miniBatch, eta) #calls a function that updates the weights and biases of the network according to a single iteration of gradient descent useing just the data in the miniBatch\n",
    "            if testData:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(testData), nTest))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "                \n",
    "    def updateMiniBatch(self, miniBatch, eta):\n",
    "        #\"Update the network's weights and biases applying gradient descent using backpropagation to a single mini batch\"\n",
    "        #The 'miniBatch' is a list of tuples and 'eta' is the learning rate\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] #gradient of biases\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] #gradient of weights\n",
    "        for x,y in miniBatch: #computes the gradient for every training example on miniBatch\n",
    "            delta_nabla_b, delta_nabla_w = self.backdrop(x,y) #invokes the backpropagation algorithm (!!!)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] #updates the direction of the gradient vector\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(miniBatch))*nw for w,nw in zip(self.weights, nabla_w)] #updates the weights\n",
    "        self.biases = [b-(eta/len(miniBatch))*nb for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code for the \"backdrop\" function is still missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
