{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "#derivative of the sigmoid function\n",
    "def sigmoidPrime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "#Main class that represents the network\n",
    "class Network(object):\n",
    "    def __init__(self, sizes): #sizes is a list that contains the number of neurons in each respective layer\n",
    "        self.numLayers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]] #randomly initializing the biases after the first layer (because the first one is the input layer) with values as per standard normal distribution\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] #randomly initializing the weights as a 2d matrix where the x axis symbolizes how many neurons are in that layer and the y axis the value of each connection\n",
    "        \n",
    "    def feedForward(self, a):\n",
    "        \"Return the output of the network if 'a' is input.\"\n",
    "        for b, w in zip(self.biases, self.weights): #zip aggregates iterables into tuples\n",
    "            a = sigmoid(np.dot(w,a)+b) #np.dot = produto vetorial\n",
    "            \n",
    "    def SGD(self, training_data, epochs, miniBatchSize, eta, test_data = None): #Stochastic gradient descent\n",
    "        #eta is the learning rate n\n",
    "        #\"Train the neural network using mini-batch stochastic gradient descent. the training_data is a list of tuples representing the training inputs and the desired outputs.\"\n",
    "        #\"If test_data is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. Thi is useful for tracking progress but it slows things down substantialy\"\n",
    "        if test_data: nTest = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data) #shuffle the order of training_data\n",
    "            #now that the training_data has been shuffled we will partition it in miniBatches\n",
    "            miniBatches = [training_data[k:k+miniBatchSize] for k in range(0, n, miniBatchSize)] #This range goes from 0 to n in miniBatchSize jumps\n",
    "            for miniBatch in miniBatches:\n",
    "                self.updateMiniBatch(miniBatch, eta) #calls a function that updates the weights and biases of the network according to a single iteration of gradient descent useing just the data in the miniBatch\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), nTest))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "                \n",
    "    #this function updates the weights and biases of the network by computing the gradient of the current mini batch\n",
    "    def updateMiniBatch(self, miniBatch, eta):\n",
    "        #\"Update the network's weights and biases applying gradient descent using backpropagation to a single mini batch\"\n",
    "        #The 'miniBatch' is a list of tuples and 'eta' is the learning rate\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] #initializing gradient of biases with all zeroes\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] #gradient of weights\n",
    "        for x,y in miniBatch: #computes the gradient for every training example on miniBatch\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y) #invokes the backpropagation algorithm and returns a new gradient vector for w and b\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] #updates the direction of the gradient vector\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(miniBatch))*nw for w,nw in zip(self.weights, nabla_w)] #updates the weights\n",
    "        self.biases = [b-(eta/len(miniBatch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self,x ,y):\n",
    "        #\"Return a tuple (nabla_b, nabla_w) representing the gradient cost function C(x)\"\n",
    "        #nabla_b and nabla_w are layer-by-layer lists of numpy arrays, similar to self.biases and self.weights\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] # .shape method takes the dimension of the object that called it\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] #np.zeros(w.shape) basically means \"instantiate an array of zeroes with the dimensions of 'w'\"\n",
    "        #feedforward - generates new activations for each neuron\n",
    "        activation = x #x is the activation value of each neuron\n",
    "        activations = [x] #list to store all the activations, layer by layer\n",
    "        zs = [] #list to store all the z vectors, layer by layer. Z vector is the value of 'w.a + b'\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b #this is what goes inside the sigmoid function\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        #backward pass - updates the weights and biases of the network from back-to-front\n",
    "        delta = self.costDerivative(activations[-1], y) * sigmoidPrime(zs[-1]) #negative indexes are suppose to be counted from the right to the left, ie, -1 is the last element of the array\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) #-2 is the second last element of the array\n",
    "        #The variable 'l' on the loop below is to be interpreted as follow:\n",
    "        #l = 1 means the last layer, l = 2 is the second last layer and so on.\n",
    "        for l in range(2, self.numLayers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoidPrime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self,test_data):\n",
    "        # Returns the number of test inputs for which the neural network outputs the correct result.\n",
    "        testResults = [(np.argmax(self.feedForward(x)), y) for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x,y) in testResults)\n",
    "    \n",
    "    def costDerivative(self, outputActivations, y):\n",
    "        # Return the vector of partial derivatives d(C(x))/d(a)\n",
    "        return (outputActivations-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data():\n",
    "    f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "test_data = list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 980 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 10, 30, 10, test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
